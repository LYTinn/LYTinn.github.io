<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LYTinn</title>
  
  <subtitle>My learning notes</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-09-04T05:38:08.119Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Run</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Neuron Layers</title>
    <link href="http://example.com/2022/09/04/Neuron-Layers/"/>
    <id>http://example.com/2022/09/04/Neuron-Layers/</id>
    <published>2022-09-04T01:00:58.000Z</published>
    <updated>2022-09-04T05:38:08.119Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Neuron-Layers"><a href="#Neuron-Layers" class="headerlink" title="Neuron Layers"></a>Neuron Layers</h1><ul><li><a href="#neuron-layers">Neuron Layers</a><ul><li><a href="#weight-matrix-of-a-layer">Weight matrix of a layer</a></li><li><a href="#synaptic-input-at-a-layer-for-single-input">Synaptic input at a layer for single input</a></li><li><a href="#synaptic-input-to-a-layer-for-batch-input">Synaptic input to a layer for batch input</a></li><li><a href="#activation-at-a-layer-for-batch-input">Activation at a layer for batch input</a></li><li><a href="#sgd-for-single-layer">SGD for single layer</a></li><li><a href="#gd-for-single-layer">GD for single layer</a></li></ul></li></ul><h2 id="Weight-matrix-of-a-layer"><a href="#Weight-matrix-of-a-layer" class="headerlink" title="Weight matrix of a layer"></a>Weight matrix of a layer</h2><p>Consider a layer of K neurons:<br><img src="../figures/K%20neurons.png" alt="K neurons"></p><p>Let $\mathbf{W_k}$ and $b_k$ denote the weight vector and bias of $k$ th neuron. Weights connected to a neuron layer is represented by a weight matrix $\mathbf{W}$ where the columns are given by weight vectors of individual neurons:</p><script type="math/tex; mode=display">\mathbf{W} = (\mathbf{w_1}\quad\mathbf{w_2}\quad\dots\quad\mathbf{w_k})</script><p>and a bias vector $\mathbf{b}$ where each element corresponds to a bias of a neuron:</p><script type="math/tex; mode=display">\mathbf{b} = (b_1, b_2, \dots, b_K)^T</script><h2 id="Synaptic-input-at-a-layer-for-single-input"><a href="#Synaptic-input-at-a-layer-for-single-input" class="headerlink" title="Synaptic input at a layer for single input"></a>Synaptic input at a layer for single input</h2><p>Given an input pattern $x\in \mathbb{R}^n$ to a layer of $K$ neurons. The synaptic input $u_k$ to $k$ th neuron is:</p><script type="math/tex; mode=display">u_k = \mathbf{w}_k^T\mathbf{x}+ b_k</script><p>where $\mathbf{w}_k$ and $b_k$ denote the weight vector and bias of $k$th neuron. Synaptic input vector $u$ to the layer is:</p><script type="math/tex; mode=display">\mathbf{u} = \left(\begin{array}{cc}    u_1\\u_2\\\vdots\\u_k\end{array}\right) = \left(\begin{array}{cc}    \mathbf{w}_1^T\mathbf{x} + b_1\\    \mathbf{w}_2^T\mathbf{x} + b_2\\    \vdots\\    \mathbf{w}_k^T\mathbf{x} + b_k\end{array}\right) = \left(\begin{array}{cc}    \mathbf{w}_1^T\\    \mathbf{w}_2^T\\    \vdots\\    \mathbf{w}_k^T\\\end{array}\right)\mathbf{x} + \left(\begin{array}{cc}    b_1\\b_2\\\vdots\\b_k\end{array}\right) = \mathbf{W}^T\mathbf{x} + \mathbf{b}</script><p>where $\mathbf{W}$ is the weight matrix and $\mathbf{b}$ is the bias vector of the layer.</p><h2 id="Synaptic-input-to-a-layer-for-batch-input"><a href="#Synaptic-input-to-a-layer-for-batch-input" class="headerlink" title="Synaptic input to a layer for batch input"></a>Synaptic input to a layer for batch input</h2><p>Given a set ${\mathbf{x}<em>p}</em>{p=1}^P$ input patterns to a layer of $K$ neurons where $\mathbf{x}_p\in \mathbb{R}^n$.</p><p>Synaptic input $\mathbf{u}_p$ to the layer for an input pattern $\mathbf{x}_p$ is:</p><script type="math/tex; mode=display">\mathbf{u}_p = \mathbf{W}^T\mathbf{x}_p + \mathbf{b}</script><p>The synaptic input matrix $\mathbf{U}$ to the layer for P patterns:</p><script type="math/tex; mode=display">\mathbf{U} = \left(\begin{array}{cc}    \mathbf{x}_1^T\mathbf{W} + \mathbf{b}^T\\    \mathbf{x}_2^T\mathbf{W} + \mathbf{b}^T\\    \vdots\\    \mathbf{x}_P^T\mathbf{W} + \mathbf{b}^T\end{array}\right) = \left(\begin{array}{cc}    \mathbf{x}_1^T\\    \mathbf{x}_2^T\\    \vdots\\    \mathbf{x}_P^T\end{array}\right)\mathbf{W} + \left(\begin{array}{cc}    \mathbf{b}^T\\\mathbf{b}^T\\\vdots\\\mathbf{b}^T\end{array}\right) = \mathbf{XW} + \mathbf{B}</script><p>where rows $\mathbf{U}$ are synaptic inputs corresponding to individual input patterns.</p><p>The matrix </p><script type="math/tex; mode=display">\mathbf{B} = \left(\begin{array}{cc}    \mathbf{b}^T\\\mathbf{b}^T\\\vdots\\\mathbf{b}^T\end{array}\right)</script><p>has bias vector propagated as rows.</p><p>Using batch in deep learning could accerate the speed of training.</p><h2 id="Activation-at-a-layer-for-batch-input"><a href="#Activation-at-a-layer-for-batch-input" class="headerlink" title="Activation at a layer for batch input"></a>Activation at a layer for batch input</h2><p>Activation of the layer of synaptic input to the layer due to a batch of patterns:</p><script type="math/tex; mode=display">f(\mathbf{U}) = \left(\begin{array}{cc}    f(\mathbf{u}_1^T) \\ f(\mathbf{u}_2^T) \\\vdots \\f(\mathbf{u}_P^T)\end{array}\right) = \left(\begin{array}{cc}    f(\mathbf{u}_1)^T\\    f(\mathbf{u}_2)^T\\    \vdots\\    f(\mathbf{u}_P)^T\end{array}\right)</script><p>where activation of each pattern is written as rows.</p><h2 id="SGD-for-single-layer"><a href="#SGD-for-single-layer" class="headerlink" title="SGD for single layer"></a>SGD for single layer</h2><p>Computational graph for processing input $(\mathbf{x}, \mathbf{d})$:<br><img src="../figures/SGD-for-single-layer.png" alt="SGD for single layer"><br>$J$ denotes the cost function. Now, we need to compute gradients $\nabla<em>\mathbf{W}J$ and $\nabla</em>\mathbf{b}J$ to learn weight matrix $\mathbf{W}$ and bias vector $\mathbf{b}$.</p><p>Consider $k$th neuron at the layer:</p><script type="math/tex; mode=display">u_k = \mathbf{w}_k^T\mathbf{x} + b_k = \sum_{i=0}^dw_{ki}x_i + b_k</script><p>where d is the rank of $\mathbf{x}$. So that</p><script type="math/tex; mode=display">\begin{aligned}    &\frac{\partial u_k}{\partial\mathbf{W}_k}\\    =& \left(\begin{array}{cc}        \frac{\partial (\sum_{i=0}^dw_{ki}x_i + b_k)}{\partial w_{k1}}\\        \frac{\partial (\sum_{i=0}^dw_{ki}x_i + b_k)}{\partial w_{k2}}\\        \vdots\\        \frac{\partial (\sum_{i=0}^dw_{ki}x_i + b_k)}{\partial w_{kd}}    \end{array}\right)\\    =&\left(\begin{array}{cc}        x_1\\x_2\\\vdots\\x_d    \end{array}\right)\\    =&\mathbf{x}\end{aligned}</script><p>The gradient of the cost with respect to the weight connected to $k$th neuron is:</p><script type="math/tex; mode=display">\begin{equation}    \nabla_{\mathbf{w}_k}J = \frac{\partial J}{\partial u_k}\frac{\partial u_k}{\partial \mathbf{w}_k} = \mathbf{x}\frac{\partial J}{\partial u_k}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}    \nabla_{b_k}J = \frac{\partial J}{\partial u_k}\frac{\partial u_k}{\partial b_k} = \frac{\partial J}{\partial u_k}\end{equation}</script><p>The gradient of $J$ with respect to $\mathbf{W} = (\mathbf{w_1}\quad \mathbf{w_2}\quad\dots\quad\mathbf{w}_k)$:</p><script type="math/tex; mode=display">\begin{aligned}    \nabla_wJ &= (\nabla_{\mathbf{w}1}J\quad\nabla_{\mathbf{w}2}J\quad\dots\quad\nabla_{\mathbf{w}K}J)\\    &=\left(\mathbf{x}\frac{\partial J}{\partial u_1}\quad\mathbf{x}\frac{\partial J}{\partial u_2}\quad\dots\quad\mathbf{x}\frac{\partial J}{\partial u_K}\right)\\    &=\mathbf{x}\left(\frac{\partial J}{\partial u_1}\quad\frac{\partial J}{\partial u_2}\quad\dots\quad\frac{\partial J}{\partial u_K}\right)\\    &=\mathbf{x}(\nabla_\mathbf{u}J)^T\end{aligned}</script><p>where</p><script type="math/tex; mode=display">\nabla_\mathbf{u}J = \frac{\partial J}{\partial \mathbf{u}} = \left(\begin{array}{cc}    \frac{\partial J}{\partial u_1}\\\frac{\partial J}{\partial u_2}\\\vdots\\\frac{\partial J}{\partial u_K}\end{array}\right)</script><p>Similarly, by substituting $\frac{\partial J}{\partial b_k} = \frac{\partial J}{\partial u_k}$ from (2):</p><script type="math/tex; mode=display">\begin{equation}    \nabla_\mathbf{b}J = \left(\begin{array}{cc}        \frac{\partial J}{\partial b_1}\\        \frac{\partial J}{\partial b_2}\\\vdots\\        \frac{\partial J}{\partial b_K}    \end{array}\right) = \left(\begin{array}{cc}        \frac{\partial J}{\partial u_1}\\        \frac{\partial J}{\partial u_2}\\\vdots\\        \frac{\partial J}{\partial u_K}    \end{array}\right) = \nabla_\mathbf{u}J\end{equation}</script><p>Thus, for any income data $\mathbf{x}$, the gradients of $\mathbf{w}$ and $b$ are:</p><script type="math/tex; mode=display">\nabla_\mathbf{w}J = \mathbf{x}(\nabla_\mathbf{u}J)^T\\\nabla_\mathbf{b}J = \nabla_\mathbf{u}J</script><p>That is, by computing gradient $\nabla_\mathbf{u}J$ with respect to synaptic input $\mathbf{u}$, the gradient of cost J with respect to the weights and biases is obtained.</p><h2 id="GD-for-single-layer"><a href="#GD-for-single-layer" class="headerlink" title="GD for single layer"></a>GD for single layer</h2><p>Given a set of patterns ${(\mathbf{x}<em>p, \mathbf{d}_p)}</em>{p=1}^P$ where $\mathbf{x}_p\in \mathbb{R}^n$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Neuron-Layers&quot;&gt;&lt;a href=&quot;#Neuron-Layers&quot; class=&quot;headerlink&quot; title=&quot;Neuron Layers&quot;&gt;&lt;/a&gt;Neuron Layers&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#neuron-lay</summary>
      
    
    
    
    <category term="Deep Learning" scheme="http://example.com/categories/Deep-Learning/"/>
    
    
    <category term="Neuron Layers" scheme="http://example.com/tags/Neuron-Layers/"/>
    
  </entry>
  
</feed>
